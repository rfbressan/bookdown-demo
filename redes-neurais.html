<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 10 Precificação de Opções via Redes Neurais | Introdução a Superfície de Volatilidade</title>
  <meta name="description" content="Gitbook sobre superfícies de volatilidade para o Clube de Finanças ESAG." />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 10 Precificação de Opções via Redes Neurais | Introdução a Superfície de Volatilidade" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Gitbook sobre superfícies de volatilidade para o Clube de Finanças ESAG." />
  <meta name="github-repo" content="rfbressan/vol_surface_gitbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 10 Precificação de Opções via Redes Neurais | Introdução a Superfície de Volatilidade" />
  
  <meta name="twitter:description" content="Gitbook sobre superfícies de volatilidade para o Clube de Finanças ESAG." />
  

<meta name="author" content="Rafael Felipe Bressan" />


<meta name="date" content="2019-07-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="monte-carlo.html">
<link rel="next" href="bibliografia.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introdução a Superfície de Volatilidade</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefácio</a></li>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html"><i class="fa fa-check"></i>Sobre os Autores</a><ul>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html#rafael-felipe-bressan"><i class="fa fa-check"></i>Rafael Felipe Bressan</a></li>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html#erik-naoki-kawano"><i class="fa fa-check"></i>Erik Naoki Kawano</a></li>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html#glauber-naue"><i class="fa fa-check"></i>Glauber Naue</a></li>
<li class="chapter" data-level="" data-path="sobre-os-autores.html"><a href="sobre-os-autores.html#gabriel-dias"><i class="fa fa-check"></i>Gabriel Dias</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="opcoes.html"><a href="opcoes.html"><i class="fa fa-check"></i><b>1</b> Introdução as opções</a><ul>
<li class="chapter" data-level="1.1" data-path="opcoes.html"><a href="opcoes.html#conceitos-in-the-money-at-the-money-e-out-the-money"><i class="fa fa-check"></i><b>1.1</b> Conceitos in the money, at the money e out the money</a></li>
<li class="chapter" data-level="1.2" data-path="opcoes.html"><a href="opcoes.html#modelos-americano-e-europeu-de-opcoes"><i class="fa fa-check"></i><b>1.2</b> Modelos americano e europeu de opções</a></li>
<li class="chapter" data-level="1.3" data-path="opcoes.html"><a href="opcoes.html#hedge"><i class="fa fa-check"></i><b>1.3</b> Hedge</a></li>
<li class="chapter" data-level="1.4" data-path="opcoes.html"><a href="opcoes.html#travas"><i class="fa fa-check"></i><b>1.4</b> Travas</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="processos-estocasticos.html"><a href="processos-estocasticos.html"><i class="fa fa-check"></i><b>2</b> Processos Estocásticos em Finanças</a><ul>
<li class="chapter" data-level="2.1" data-path="processos-estocasticos.html"><a href="processos-estocasticos.html#markov"><i class="fa fa-check"></i><b>2.1</b> Processos de Markov</a></li>
<li class="chapter" data-level="2.2" data-path="processos-estocasticos.html"><a href="processos-estocasticos.html#mb"><i class="fa fa-check"></i><b>2.2</b> Movimento Browniano</a></li>
<li class="chapter" data-level="2.3" data-path="processos-estocasticos.html"><a href="processos-estocasticos.html#definicao"><i class="fa fa-check"></i><b>2.3</b> Definição</a></li>
<li class="chapter" data-level="2.4" data-path="processos-estocasticos.html"><a href="processos-estocasticos.html#mbg"><i class="fa fa-check"></i><b>2.4</b> Movimento Browniano Geométrico</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bsm.html"><a href="bsm.html"><i class="fa fa-check"></i><b>3</b> Modelo Black-Scholes-Merton</a><ul>
<li class="chapter" data-level="3.1" data-path="bsm.html"><a href="bsm.html#portfolio-de-replicacao"><i class="fa fa-check"></i><b>3.1</b> Portfólio de replicação</a></li>
<li class="chapter" data-level="3.2" data-path="bsm.html"><a href="bsm.html#precificacao-neutra-ao-risco"><i class="fa fa-check"></i><b>3.2</b> Precificação neutra ao risco</a></li>
<li class="chapter" data-level="3.3" data-path="bsm.html"><a href="bsm.html#encontrando-a-equacao-de-black-scholes"><i class="fa fa-check"></i><b>3.3</b> Encontrando a equação de Black-Scholes</a></li>
<li class="chapter" data-level="3.4" data-path="bsm.html"><a href="bsm.html#solucao-analitica"><i class="fa fa-check"></i><b>3.4</b> Solução analítica</a></li>
<li class="chapter" data-level="3.5" data-path="bsm.html"><a href="bsm.html#putcallparity"><i class="fa fa-check"></i><b>3.5</b> Paridade compra e venda</a></li>
<li class="chapter" data-level="3.6" data-path="bsm.html"><a href="bsm.html#gregas"><i class="fa fa-check"></i><b>3.6</b> As Gregas</a><ul>
<li class="chapter" data-level="3.6.1" data-path="bsm.html"><a href="bsm.html#delta"><i class="fa fa-check"></i><b>3.6.1</b> Delta</a></li>
<li class="chapter" data-level="3.6.2" data-path="bsm.html"><a href="bsm.html#gamma"><i class="fa fa-check"></i><b>3.6.2</b> Gamma</a></li>
<li class="chapter" data-level="3.6.3" data-path="bsm.html"><a href="bsm.html#theta"><i class="fa fa-check"></i><b>3.6.3</b> Theta</a></li>
<li class="chapter" data-level="3.6.4" data-path="bsm.html"><a href="bsm.html#vega"><i class="fa fa-check"></i><b>3.6.4</b> Vega</a></li>
<li class="chapter" data-level="3.6.5" data-path="bsm.html"><a href="bsm.html#rho"><i class="fa fa-check"></i><b>3.6.5</b> Rho</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="smile.html"><a href="smile.html"><i class="fa fa-check"></i><b>4</b> Smile de Volatilidade</a><ul>
<li class="chapter" data-level="4.1" data-path="smile.html"><a href="smile.html#reparametrizando"><i class="fa fa-check"></i><b>4.1</b> Reparametrizando B&amp;S e definição de moneyness</a></li>
<li class="chapter" data-level="4.2" data-path="smile.html"><a href="smile.html#caracsmile"><i class="fa fa-check"></i><b>4.2</b> Características de smiles de volatilidade</a><ul>
<li class="chapter" data-level="4.2.1" data-path="smile.html"><a href="smile.html#mercados-cambiais"><i class="fa fa-check"></i><b>4.2.1</b> Mercados cambiais</a></li>
<li class="chapter" data-level="4.2.2" data-path="smile.html"><a href="smile.html#mercados-de-equities"><i class="fa fa-check"></i><b>4.2.2</b> Mercados de <em>equities</em></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="smile.html"><a href="smile.html#smileprecificacao"><i class="fa fa-check"></i><b>4.3</b> Smile como forma de precificação</a></li>
<li class="chapter" data-level="4.4" data-path="smile.html"><a href="smile.html#estrutura-a-termo"><i class="fa fa-check"></i><b>4.4</b> Estrutura a termo</a></li>
<li class="chapter" data-level="4.5" data-path="smile.html"><a href="smile.html#arbestatica"><i class="fa fa-check"></i><b>4.5</b> Arbitragem estática</a><ul>
<li class="chapter" data-level="4.5.1" data-path="smile.html"><a href="smile.html#arbitragem-de-borboleta"><i class="fa fa-check"></i><b>4.5.1</b> Arbitragem de borboleta</a></li>
<li class="chapter" data-level="4.5.2" data-path="smile.html"><a href="smile.html#arbitragem-de-calendario"><i class="fa fa-check"></i><b>4.5.2</b> Arbitragem de calendário</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="smile.html"><a href="smile.html#limites-de-inclinacao"><i class="fa fa-check"></i><b>4.6</b> Limites de inclinação</a></li>
<li class="chapter" data-level="4.7" data-path="smile.html"><a href="smile.html#rnd"><i class="fa fa-check"></i><b>4.7</b> Distribuição implícita</a></li>
<li class="chapter" data-level="4.8" data-path="smile.html"><a href="smile.html#conclusao"><i class="fa fa-check"></i><b>4.8</b> Conclusão</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="superficies.html"><a href="superficies.html"><i class="fa fa-check"></i><b>5</b> Superfícies de volatilidade</a><ul>
<li class="chapter" data-level="5.1" data-path="superficies.html"><a href="superficies.html#modelos-estocasticos"><i class="fa fa-check"></i><b>5.1</b> Modelos estocásticos</a><ul>
<li class="chapter" data-level="5.1.1" data-path="superficies.html"><a href="superficies.html#heston"><i class="fa fa-check"></i><b>5.1.1</b> Heston</a></li>
<li class="chapter" data-level="5.1.2" data-path="superficies.html"><a href="superficies.html#sabr"><i class="fa fa-check"></i><b>5.1.2</b> SABR</a></li>
<li class="chapter" data-level="5.1.3" data-path="superficies.html"><a href="superficies.html#bates"><i class="fa fa-check"></i><b>5.1.3</b> Bates</a></li>
<li class="chapter" data-level="5.1.4" data-path="superficies.html"><a href="superficies.html#volatilidade-local"><i class="fa fa-check"></i><b>5.1.4</b> Volatilidade local</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="superficies.html"><a href="superficies.html#modelos-parametricos"><i class="fa fa-check"></i><b>5.2</b> Modelos paramétricos</a><ul>
<li class="chapter" data-level="5.2.1" data-path="superficies.html"><a href="superficies.html#sec:svi"><i class="fa fa-check"></i><b>5.2.1</b> SVI</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="superficies.html"><a href="superficies.html#modelos-nao-parametricos"><i class="fa fa-check"></i><b>5.3</b> Modelos não-paramétricos</a><ul>
<li class="chapter" data-level="5.3.1" data-path="superficies.html"><a href="superficies.html#interpolacao-e-suavizacao-spline"><i class="fa fa-check"></i><b>5.3.1</b> Interpolação e suavização spline</a></li>
<li class="chapter" data-level="5.3.2" data-path="superficies.html"><a href="superficies.html#algoritmos-livres-de-arbitragem"><i class="fa fa-check"></i><b>5.3.2</b> Algoritmos livres de arbitragem</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="superficies.html"><a href="superficies.html#extrapolacao-do-smile"><i class="fa fa-check"></i><b>5.4</b> Extrapolação do smile</a></li>
<li class="chapter" data-level="5.5" data-path="superficies.html"><a href="superficies.html#conclusao-1"><i class="fa fa-check"></i><b>5.5</b> Conclusão</a></li>
<li class="chapter" data-level="5.6" data-path="superficies.html"><a href="superficies.html#referencias"><i class="fa fa-check"></i><b>5.6</b> Referências</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="calibracao.html"><a href="calibracao.html"><i class="fa fa-check"></i><b>6</b> Métodos de Calibração</a><ul>
<li class="chapter" data-level="6.1" data-path="calibracao.html"><a href="calibracao.html#calibracao-versus-interpolacao"><i class="fa fa-check"></i><b>6.1</b> Calibração versus Interpolação</a></li>
<li class="chapter" data-level="6.2" data-path="calibracao.html"><a href="calibracao.html#spline-cubica"><i class="fa fa-check"></i><b>6.2</b> Spline cúbica</a></li>
<li class="chapter" data-level="6.3" data-path="calibracao.html"><a href="calibracao.html#suavizacao"><i class="fa fa-check"></i><b>6.3</b> Suavização</a></li>
<li class="chapter" data-level="6.4" data-path="calibracao.html"><a href="calibracao.html#parametrizacao"><i class="fa fa-check"></i><b>6.4</b> Parametrização</a><ul>
<li class="chapter" data-level="6.4.1" data-path="calibracao.html"><a href="calibracao.html#funcao-perda"><i class="fa fa-check"></i><b>6.4.1</b> Função perda</a></li>
<li class="chapter" data-level="6.4.2" data-path="calibracao.html"><a href="calibracao.html#otimizadores"><i class="fa fa-check"></i><b>6.4.2</b> Otimizadores</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="calibracao.html"><a href="calibracao.html#conclusao-2"><i class="fa fa-check"></i><b>6.5</b> Conclusão</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="svi.html"><a href="svi.html"><i class="fa fa-check"></i><b>7</b> Calibrando uma SVI</a><ul>
<li class="chapter" data-level="7.1" data-path="svi.html"><a href="svi.html#modelo-svi"><i class="fa fa-check"></i><b>7.1</b> Modelo SVI</a></li>
<li class="chapter" data-level="7.2" data-path="svi.html"><a href="svi.html#restricoes-de-nao-arbitragem"><i class="fa fa-check"></i><b>7.2</b> Restrições de não-arbitragem</a></li>
<li class="chapter" data-level="7.3" data-path="svi.html"><a href="svi.html#reparametrizacao-quasi-explicit"><i class="fa fa-check"></i><b>7.3</b> Reparametrização Quasi-explicit</a><ul>
<li class="chapter" data-level="7.3.1" data-path="svi.html"><a href="svi.html#pq"><i class="fa fa-check"></i><b>7.3.1</b> Solução explícita do problema reduzido</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="svi.html"><a href="svi.html#algoritmo"><i class="fa fa-check"></i><b>7.4</b> Algoritmo</a></li>
<li class="chapter" data-level="7.5" data-path="svi.html"><a href="svi.html#resultados"><i class="fa fa-check"></i><b>7.5</b> Resultados</a></li>
<li class="chapter" data-level="7.6" data-path="svi.html"><a href="svi.html#conclusao-3"><i class="fa fa-check"></i><b>7.6</b> Conclusão</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ssvi.html"><a href="ssvi.html"><i class="fa fa-check"></i><b>8</b> Superfície SVI</a><ul>
<li class="chapter" data-level="8.1" data-path="ssvi.html"><a href="ssvi.html#reparametrizacoes-equivalentes"><i class="fa fa-check"></i><b>8.1</b> Reparametrizações equivalentes</a></li>
<li class="chapter" data-level="8.2" data-path="ssvi.html"><a href="ssvi.html#superficie-svi"><i class="fa fa-check"></i><b>8.2</b> Superfície SVI</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ssvi.html"><a href="ssvi.html#heston-1"><i class="fa fa-check"></i><b>8.2.1</b> Heston</a></li>
<li class="chapter" data-level="8.2.2" data-path="ssvi.html"><a href="ssvi.html#lei-de-potencia"><i class="fa fa-check"></i><b>8.2.2</b> Lei de potência</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ssvi.html"><a href="ssvi.html#condicoes-de-nao-arbitragem"><i class="fa fa-check"></i><b>8.3</b> Condições de não-arbitragem</a></li>
<li class="chapter" data-level="8.4" data-path="ssvi.html"><a href="ssvi.html#densidade-neutra-ao-risco"><i class="fa fa-check"></i><b>8.4</b> Densidade neutra ao risco</a></li>
<li class="chapter" data-level="8.5" data-path="ssvi.html"><a href="ssvi.html#superficie-de-volatilidade-local"><i class="fa fa-check"></i><b>8.5</b> Superfície de volatilidade local</a></li>
<li class="chapter" data-level="8.6" data-path="ssvi.html"><a href="ssvi.html#calibracao-da-ssvi"><i class="fa fa-check"></i><b>8.6</b> Calibração da SSVI</a></li>
<li class="chapter" data-level="8.7" data-path="ssvi.html"><a href="ssvi.html#conclusao-4"><i class="fa fa-check"></i><b>8.7</b> Conclusão</a></li>
<li class="chapter" data-level="8.8" data-path="ssvi.html"><a href="ssvi.html#referencias-1"><i class="fa fa-check"></i><b>8.8</b> Referências</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="monte-carlo.html"><a href="monte-carlo.html"><i class="fa fa-check"></i><b>9</b> Simulação de Monte Carlo</a><ul>
<li class="chapter" data-level="9.1" data-path="monte-carlo.html"><a href="monte-carlo.html#simulacao-de-monte-carlo"><i class="fa fa-check"></i><b>9.1</b> Simulação de Monte Carlo</a></li>
<li class="chapter" data-level="9.2" data-path="monte-carlo.html"><a href="monte-carlo.html#implementacao-em-r"><i class="fa fa-check"></i><b>9.2</b> Implementação em R</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="redes-neurais.html"><a href="redes-neurais.html"><i class="fa fa-check"></i><b>10</b> Precificação de Opções via Redes Neurais</a><ul>
<li class="chapter" data-level="10.1" data-path="redes-neurais.html"><a href="redes-neurais.html#hipoteses-de-black-scholes"><i class="fa fa-check"></i><b>10.1</b> Hipóteses de Black &amp; Scholes</a></li>
<li class="chapter" data-level="10.2" data-path="redes-neurais.html"><a href="redes-neurais.html#ensinando-uma-maquina-a-precificar-opcoes"><i class="fa fa-check"></i><b>10.2</b> Ensinando uma máquina a precificar opções</a></li>
<li class="chapter" data-level="10.3" data-path="redes-neurais.html"><a href="redes-neurais.html#redes-neurais"><i class="fa fa-check"></i><b>10.3</b> Redes Neurais</a></li>
<li class="chapter" data-level="10.4" data-path="redes-neurais.html"><a href="redes-neurais.html#feed-forward-network"><i class="fa fa-check"></i><b>10.4</b> Feed forward network</a></li>
<li class="chapter" data-level="10.5" data-path="redes-neurais.html"><a href="redes-neurais.html#backpropagation"><i class="fa fa-check"></i><b>10.5</b> Backpropagation</a></li>
<li class="chapter" data-level="10.6" data-path="redes-neurais.html"><a href="redes-neurais.html#rede-neural-no-r"><i class="fa fa-check"></i><b>10.6</b> Rede neural no R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografia.html"><a href="bibliografia.html"><i class="fa fa-check"></i>Bibliografia</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org/" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introdução a Superfície de Volatilidade</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="redes-neurais" class="section level1">
<h1><span class="header-section-number">Capítulo 10</span> Precificação de Opções via Redes Neurais</h1>
<p>Introduziremos neste capítulo<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> um dos temas mais atuais no mercado financeiro. O uso de técnicas de inteligência artificial e aprendizado de máquina para a precificação de opções.</p>
<p>Em 1973, Fischer Black, Myron Scholes e Robert Merton publicaram uma maneira analítica para precificar opções, de modo que se pudesse relacionar a dinâmica do preço do ativo subjacente, uma taxa de juros livre de risco, o intervalo de tempo até a data de expiração da opção, o strike da opção e a volatilidade futura deste ativo, sem depender de um retorno esperado (algo muito mais imprevisível).</p>
<div id="hipoteses-de-black-scholes" class="section level2">
<h2><span class="header-section-number">10.1</span> Hipóteses de Black &amp; Scholes</h2>
<p>Como já vimos em artigos anteriores, a fórmula de precificação de Black &amp; Scholes <span class="citation">(Black and Scholes <a href="#ref-Black1973">1973</a>)</span> assumia a hipótese de lognormalidade para os retornos do ativo subjacente, além de que a volatilidade implícita do ativo se manteria constante para opções de um mesmo ativo e de mesmo moneyness. Em relação a hipótese de lognormalidade, isso nos diz que temos um método paramétrico de precificação, o que pode ser ruim (temos que assumir uma distribuição que não se ajusta com o que acontece nos mercados, que apresentam caudas longas), e a volatilidade implícita constante também não se concretiza, dando origem ao fenômeno chamado de “smile de volatilidade”, algo também já tratado nos nossos artigos antigos.</p>
</div>
<div id="ensinando-uma-maquina-a-precificar-opcoes" class="section level2">
<h2><span class="header-section-number">10.2</span> Ensinando uma máquina a precificar opções</h2>
<p>Na década de 70, seria muito difícil construir um método prático e data-driven de precificação de opções, já que não se possuía poder computacional suficiente para realizar a mineração necessária; tínhamos que ter uma maneira analítica de precificação, o que começou com a fórmula de Black &amp; Scholes (que garantiu o Prêmio Nobel aos pesquisadores). A partir da década de 90, com o avanço computacional, alguns pesquisadores começaram a se interessar por métodos data-driven de precificação, podendo se desvencilhar das hipóteses pouco realistas.</p>
<p>Para tanto, podemos pensar em algumas possibilidades: tendo informações sobre as características de uma determinada opção (o seu preço de mercado, uma volatilidade implícita realizada, um determinado intervalo de tempo até a data de expiração, com o moneyness da opção, etc…), teríamos condições de ensinar um algoritmo a precificar esta opção?</p>
<p>Em <span class="citation">(Hutchinson <a href="#ref-Hutchinson1994">1994</a>)</span>, esta abordagem foi realizada com a utilização do método das redes neurais. A abordagem do pesquisadores do MIT era de ensinar uma máquina a precificar opções de maneira não-paramétrica e que não assumisse as hipóteses tão contestadas por outros pesquisadores. Colocando como input as informações teoricamente determinantes para os preços das opções, o artigo buscou ver o ajuste das previsões realizadas com o que realmente aconteceu nos mercados. Outro bom artigo, aplicando a mesma técnica mas para opções de outro mercado, é o de <span class="citation">(Huynh <a href="#ref-Huynh2018">2018</a>)</span>.</p>
</div>
<div id="redes-neurais" class="section level2">
<h2><span class="header-section-number">10.3</span> Redes Neurais</h2>
<p>Como um resumo sobre o método das redes neurais, <span class="citation">(Friedman <a href="#ref-Friedman2001">2001</a>, 389)</span> traz que a ideia é extrair combinações lineares entre as variáveis explicativas e então modelar a variável dependente (no nosso caso, o prêmio da opção) como uma função não-linear das variáveis explicativas. O modelo de redes neurais também é chamado de multilayer perceptron, onde o modelo comprime diversas camadas de regressões logísticas com não-linearidades contínuas <span class="citation">(Bishop <a href="#ref-Bishop2006">2006</a>, 226)</span>; assim é formada a função de máxima verossimilhança que é a base das “redes de treino”.</p>
<p>Ao contrário do que se pensa, a pesquisa sobre métodos de inteligência artificial, e mais especificamente de redes neurais, começou já na década de 40, com <span class="citation">(McCulloch <a href="#ref-Mcculloch1943">1943</a>)</span>, em “A Logical Calculus of The Ideas Immanent in Nervous Activity”. A ideia era de simular as redes neurais do cérebro como forma de computadores estarem aptos a aprenderem e tomarem decisões como um humano.</p>
<p>Para isso, matematicamente, construímos uma função de ativação <span class="math inline">\(y\)</span>, onde <span class="math inline">\(y\)</span> é uma função de uma função não-linear das combinações lineares entre os inputs dados (<span class="math inline">\(\phi(x)\)</span>), ponderada por pesos que, inicialmente, são aleatórios (<span class="math inline">\(w_j\)</span>), entre 0 e 1.</p>
<p><span class="math display">\[
y(\mathbf{x, w})=f\left(\sum_{j=1}^M w_j\phi_j(\mathbf{x})\right)
\]</span></p>
<p>Esses pesos, com o método de treino estipulado (backpropagation), será alterado de forma com que se alcance o erro mínimo da função para os dados inseridos. Temos que M é o número de combinações lineares, que, somados, gerarão o primeiro input para o treino da função.</p>
<div class="figure" style="text-align: center"><span id="fig:rede-neural"></span>
<img src="images/neural_network.png" alt="Típica Rede Neural com uma camada oculta."  />
<p class="caption">
Figura 10.1: Típica Rede Neural com uma camada oculta.
</p>
</div>
</div>
<div id="feed-forward-network" class="section level2">
<h2><span class="header-section-number">10.4</span> Feed forward network</h2>
<p>Agora, derivando o algoritmo para chegarmos em <span class="math inline">\(y(\mathbf{x, w})\)</span>, trazido acima:</p>
<p>1º - Primeiramente, teremos o somatório da multiplicação do vetor de pesos <span class="math inline">\(\mathbf{w}\)</span> com o vetor de inputs <span class="math inline">\(\mathbf{x}\)</span>. Temos que <span class="math inline">\(w_{ji}\)</span>é um vetor de pesos que serão alterados ao longo do treinamento da rede. Faremos esse mesmo cálculo para todas as nossas variáveis</p>
<p><span class="math display">\[
  a_{ji} = \sum_{i = 0}^{D} w_{ji}^{(1)}x_i
\]</span></p>
<p>2º - Temos de transformar o vetor de valores ponderados <span class="math inline">\(a_{ji}\)</span> através de uma função de ativação, que já é conhecido de econometristas e estudiosos da estatística: a função sigmoidal, que é a utilizada na regressão logística</p>
<p><span class="math display">\[
  \sigma(a) = \frac{1}{1 + exp(-a)}
\]</span></p>
<p>Com isso, temos então o que é chamado de “hidden layer”.</p>
<p>3º - Realizar novamente a ponderação dos valores, porém agora utilizando os hidden layers como input.</p>
<p>Logo, teremos uma nova ativação dos outputs pela função sigmoidal, dado por um input de uma função sigmoidal anterior. Nesse caso, duas camadas da rede neural foram utilizadas. Desta forma, podemos combinar estas várias etapas e chegar nesta função para a rede neural:</p>
<p><span class="math display">\[
y(\mathbf{x, w})= \sigma(\sum_{j=0}^M w_{kj}^{(2)} \sigma(\sum_{i=0}^D w_{ji}^{(1)}x_i))
\]</span></p>
</div>
<div id="backpropagation" class="section level2">
<h2><span class="header-section-number">10.5</span> Backpropagation</h2>
<p>Tendo um vetor de outputs, ou seja, valores preditos para o target utilizado, buscamos um vetor de pesos que minimize a seguinte função:</p>
<p><span class="math display">\[
E(\mathbf{w})= \frac{1}{2} \sum_{n=1}^N ||\mathbf{y(x_n, w) - t_n}||^2
\]</span></p>
<p>Ou seja, sendo <span class="math inline">\(y(\mathbf{x, w})\)</span> um vetor de outputs e <span class="math inline">\(\mathbf{t_n}\)</span> o vetor dos targets iniciais, queremos minimizar a soma dos erros quadrados. Os parâmetros que são alteráveis são os pesos, tanto da primeira camada quanto da segunda utilizada. OBS: O fator (½) é adicionado para ser cancelado junto com o expoente durante a diferenciação</p>
<p>A partir daqui, temos um problema computacional: simular infinitas possibilidades de vetores de pesos para identificar quais são os vetores que minimizam a soma do erro quadrado é uma tarefa computacionalmente exigente. Será que temos como usar a matemática para facilitar esse processo?</p>
<p>Para este problema, o método das redes neurais se utiliza do gradient descent, que é uma forma iterativa para se alcançar o mínimo de uma função.</p>
<div class="figure" style="text-align: center"><span id="fig:backpropagation"></span>
<img src="images/backpropagation.png" alt="Esquema feed-forward e backpropagation."  />
<p class="caption">
Figura 10.2: Esquema feed-forward e backpropagation.
</p>
</div>
<p>Queremos encontrar os vetores de pesos que minimizem a função erro. Assim, aplicamos a regra da cadeia para derivar a função erro:</p>
<p><span class="math display">\[
{\frac {\partial E}{\partial w_{ij}}}={\frac {\partial E}{\partial o_{j}}}{\frac {\partial o_{j}}{\partial {\text{net}}_{j}}}{\frac {\partial {\text{net}}_{j}}{\partial w_{ij}}}
\]</span> Sendo:</p>
<p><span class="math display">\[
1)
{\frac{\partial net_j}{\partial w_{ij}}}={\frac {\partial }{\partial w_{ij}}}\left(\sum _{k=1}^{n}w_{kj}o_{k}\right)
\]</span></p>
<p>que será simplesmente <span class="math inline">\(o_k\)</span>, sendo <span class="math inline">\(o_k\)</span> um vetor que se for em relação a primeira camada, o input bruto; se for em relação a segunda layer, será o output da função de ativação.</p>
<p><span class="math display">\[
2)
{\frac{\partial o_j}{\partial net_{j}}}={\frac {\partial }{\partial net_{j}}} \varphi(net_j) = \varphi(net_j)(1 - \varphi(net_j))
\]</span></p>
<p>que é a derivada parcial da função de ativação (neste caso, a função sigmoidal), e a função abaixo, que é a derivada parcial da função perda:</p>
<p><span class="math display">\[
3)
{\frac{\partial E}{\partial o_{j}}}={\frac {\partial E }{\partial y}} = {\frac {\partial}{\partial y}} {\frac {1}{2}}(t - y)^2 = y - t
\]</span></p>
<p>Assim, atualizaremos os pesos com os resultados obtidos através da otimização, e seguiremos o processo iterativo de encontrar o mínimo da função.</p>
<p>OBS: Um problema do método do gradient descent é que ele pode encontrar um mínimo local, não um mínimo global, que é o que nos interessaria. Há formas de contornar este problema, como, por exemplo, assumindo uma versão probabilística para a função perda. Dada a sua complexidade, deixaremos para artigos futuros a sua explicação. Além disso, outras formas de se alcançar melhores resultados com redes neurais para opções seria de segmentar as opções em ATM (at the money), OTM (out of the money), podendo captar melhor as características de ambas as situações.</p>
</div>
<div id="rede-neural-no-r" class="section level2">
<h2><span class="header-section-number">10.6</span> Rede neural no R</h2>
<p>Os dados foram retirados do site <a href="http://www.ivolatility.com/doc/usa/IV_Raw_Delta_surface.csv">ivolatility.com</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">##Importando os dados que serão utilizados

smile_volatilidade &lt;-<span class="st"> </span>
<span class="st">  </span>rio<span class="op">::</span><span class="kw">import</span>(<span class="st">&quot;R/input/IV_Raw_Delta_surface.csv&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(period, moneyness, iv) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">period =</span> period<span class="op">/</span><span class="dv">365</span>)

##Executando o algoritmo através do pacote neuralnet
##Foram escolhidsa duas hidden layers para análise

rede_neural &lt;-<span class="st"> </span><span class="kw">neuralnet</span>(iv <span class="op">~</span><span class="st"> </span>period<span class="op">+</span>moneyness, smile_volatilidade, <span class="dt">hidden =</span> <span class="dv">2</span>)
iv_predito &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(rede_neural<span class="op">$</span>net.result) 

df_nn &lt;-<span class="st"> </span><span class="kw">cbind</span>(smile_volatilidade<span class="op">$</span>iv, iv_predito)

<span class="kw">colnames</span>(df_nn) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;volatilidade_implicita&quot;</span>, <span class="st">&quot;volatilidade_predita&quot;</span>)

##Criando uma coluna com os erros da predição

df_nn &lt;-<span class="st"> </span>df_nn <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">erro =</span> volatilidade_implicita <span class="op">-</span><span class="st"> </span>volatilidade_predita)

<span class="kw">hist</span>(df_nn<span class="op">$</span>erro,
     <span class="dt">main =</span> <span class="st">&quot;Distribuição dos erros da volatilidade realizada pela predita&quot;</span>,
     <span class="dt">xlab =</span> <span class="st">&quot;Erro&quot;</span>,
     <span class="dt">ylab =</span> <span class="st">&quot;Frequência&quot;,</span>
<span class="st">     xlim = c(-0.04, 0.04))</span></code></pre></div>
<p><img src="10-redes-neurais_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(rede_neural, <span class="dt">rep =</span> <span class="st">&quot;best&quot;</span>)</code></pre></div>
<p><img src="10-redes-neurais_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>

</div>
</div>
<h3>Bibliografia</h3>
<div id="refs" class="references">
<div id="ref-Bishop2006">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. springer.</p>
</div>
<div id="ref-Black1973">
<p>Black, Fischer, and Myron Scholes. 1973. “The Pricing of Options and Corporate Liabilities.” <em>Journal of Political Economy</em> 81 (3). The University of Chicago Press: 637–54.</p>
</div>
<div id="ref-Friedman2001">
<p>Friedman, Trevor e Tibshirani, Jerome e Hastie. 2001. <em>The Elements of Statistical Learning</em>. 10. Springer series in statistics New York.</p>
</div>
<div id="ref-Hutchinson1994">
<p>Hutchinson, Andrew W e Poggio, James M e Lo. 1994. “A Nonparametric Approach to Pricing and Hedging Derivative Securities via Learning Networks.” <em>The Journal of Finance</em>, no. 3. Wiley Online Library: 851–89.</p>
</div>
<div id="ref-Huynh2018">
<p>Huynh. 2018. “Modelling and Forecasting Implied Volatility Using Neural Network.” Hanken School of Economics.</p>
</div>
<div id="ref-Mcculloch1943">
<p>McCulloch, Walter, Warren S e Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” <em>The Bulletin of Mathematical Biophysics</em> 5 (4). Springer: 115–33.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p>Artigo originalmente escrito por Gabriel Dias, adaptado por Rafael F. Bressan para este livro.<a href="redes-neurais.html#fnref17">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="monte-carlo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografia.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rfbressan/vol_surface_gitbook/edit/master/10-redes-neurais.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["vol_surface_book.pdf", "vol_surface_book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
